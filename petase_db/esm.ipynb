{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90e17517",
   "metadata": {},
   "source": [
    "ESM3 Tut1-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79979bc3",
   "metadata": {},
   "source": [
    "Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "091f8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from esm.models.esm3 import ESM3\n",
    "from esm.sdk.api import ESM3InferenceClient, ESMProtein, GenerationConfig\n",
    "import torch\n",
    "import esm\n",
    "from esm.sdk import client\n",
    "login(token=\"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\")\n",
    "\n",
    "device = (\n",
    "    \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "#Forge API\n",
    "model=client(\"esm3-medium-2024-08\",token=\"1hM3JVqy7zILco2fwPgF9v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b78910",
   "metadata": {},
   "source": [
    "ESM-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2063dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig\n",
    "protein = ESMProtein(sequence=\"AAAA\")\n",
    "client = ESMC.from_pretrained(\"esmc_300m\").to(\"cpu\")\n",
    "\n",
    "protein_tensor = client.encode(protein)\n",
    "logits_output = client.logits(\n",
    "    protein_tensor, LogitsConfig(sequence=True,return_embeddings=True)\n",
    ")\n",
    "print(logits_output.logits, logits_output.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5816a664",
   "metadata": {},
   "source": [
    "BATCH FORGE EXECUTOR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04a7668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from esm.sdk.forge import ESM3ForgeInferenceClient\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig, ESMProteinError, LogitsOutput\n",
    "from esm.sdk import batch_executor\n",
    "\n",
    "def embed_sequence(client:ESM3ForgeInferenceClient, sequence: str) -> LogitsOutput:\n",
    "    protein = ESMProtein(sequence=sequence)\n",
    "    protein_tensor = client.encode(protein)\n",
    "    if isinstance(protein_tensor,ESMProteinError):\n",
    "        raise protein_tensor \n",
    "    output = client.logits(protein_tensor,LogitsConfig(sequence=True, return_embeddings=True))\n",
    "    \n",
    "    return output \n",
    "sequences = [\"A\",\"AA\",\"AAA\"]\n",
    "forge_client = ESM3ForgeInferenceClient(model=\"esmc-6b-2024-12\", url=\"https://forge.evolutionaryscale.ai\", token=\"1hM3JVqy7zILco2fwPgF9v\")\n",
    "\n",
    "with batch_executor() as executor:\n",
    "    outputs = executor.execute_batch(user_func=embed_sequence, client=forge_client,sequence=sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84939492",
   "metadata": {},
   "source": [
    "Tut1: Create an ESMProtein object\n",
    "\n",
    "An ESMProtein has 5 attributes that represent input (promptable) tracks:\n",
    "\n",
    "sequence: amino acid sequence\n",
    "\n",
    "coordinates: 3D coordinates of atoms in each amino acid of the protein\n",
    "\n",
    "secondary_structure: 8-class secondary structure (SS8)\n",
    "\n",
    "sasa: solvent-accessible surface area (SASA)\n",
    "\n",
    "function_annotations: function annotations derived from InterPro\n",
    "\n",
    "You can prompt an ESM3 model by setting any subset of these tracks to be partially unmasked when calling the model with an ESMProtein instance.\n",
    "\n",
    "One way to create an ESMProtein object is from a pdb id and chain id from RCSB. Below, we first create a ProteinChain with the pdb id and chain id and then create an ESMProtein from it. This will populate the sequence and coordinates properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3efba4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for visualizing 3D structure\n",
    "\n",
    "import py3Dmol\n",
    "\n",
    "\n",
    "def visualize_pdb(pdb_string):\n",
    "    view = py3Dmol.view(width=400, height=400)\n",
    "    view.addModel(pdb_string, \"pdb\")\n",
    "    view.setStyle({\"cartoon\": {\"color\": \"spectrum\"}})\n",
    "    view.zoomTo()\n",
    "    view.render()\n",
    "    view.center()\n",
    "    return view\n",
    "\n",
    "\n",
    "def visualize_3D_coordinates(coordinates):\n",
    "    \"\"\"\n",
    "    This uses all Alanines\n",
    "    \"\"\"\n",
    "    protein_with_same_coords = ESMProtein(coordinates=coordinates)\n",
    "    # pdb with all alanines\n",
    "    pdb_string = protein_with_same_coords.to_pdb_string()\n",
    "    return visualize_pdb(pdb_string)\n",
    "\n",
    "\n",
    "def visualize_3D_protein(protein):\n",
    "    pdb_string = protein.to_pdb_string()\n",
    "    return visualize_pdb(pdb_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38accee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slightly modified version of secondary structure plotting code from\n",
    "# https://www.biotite-python.org/examples/gallery/structure/transketolase_sse.html\n",
    "# Code source: Patrick Kunzmann\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import biotite\n",
    "import biotite.sequence as seq\n",
    "import biotite.sequence.graphics as graphics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "\n",
    "# Create 'FeaturePlotter' subclasses\n",
    "# for drawing the secondary structure features\n",
    "class HelixPlotter(graphics.FeaturePlotter):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # Check whether this class is applicable for drawing a feature\n",
    "    def matches(self, feature):\n",
    "        if feature.key == \"SecStr\":\n",
    "            if \"sec_str_type\" in feature.qual:\n",
    "                if feature.qual[\"sec_str_type\"] == \"helix\":\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    # The drawing function itself\n",
    "    def draw(self, axes, feature, bbox, loc, style_param):\n",
    "        # Approx. 1 turn per 3.6 residues to resemble natural helix\n",
    "        n_turns = np.ceil((loc.last - loc.first + 1) / 3.6)\n",
    "        x_val = np.linspace(0, n_turns * 2 * np.pi, 100)\n",
    "        # Curve ranges from 0.3 to 0.7\n",
    "        y_val = (-0.4 * np.sin(x_val) + 1) / 2\n",
    "\n",
    "        # Transform values for correct location in feature map\n",
    "        x_val *= bbox.width / (n_turns * 2 * np.pi)\n",
    "        x_val += bbox.x0\n",
    "        y_val *= bbox.height\n",
    "        y_val += bbox.y0\n",
    "\n",
    "        # Draw white background to overlay the guiding line\n",
    "        background = Rectangle(\n",
    "            bbox.p0, bbox.width, bbox.height, color=\"white\", linewidth=0\n",
    "        )\n",
    "        axes.add_patch(background)\n",
    "        axes.plot(x_val, y_val, linewidth=2, color=biotite.colors[\"dimgreen\"])\n",
    "\n",
    "\n",
    "class SheetPlotter(graphics.FeaturePlotter):\n",
    "    def __init__(self, head_width=0.8, tail_width=0.5):\n",
    "        self._head_width = head_width\n",
    "        self._tail_width = tail_width\n",
    "\n",
    "    def matches(self, feature):\n",
    "        if feature.key == \"SecStr\":\n",
    "            if \"sec_str_type\" in feature.qual:\n",
    "                if feature.qual[\"sec_str_type\"] == \"sheet\":\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def draw(self, axes, feature, bbox, loc, style_param):\n",
    "        x = bbox.x0\n",
    "        y = bbox.y0 + bbox.height / 2\n",
    "        dx = bbox.width\n",
    "        dy = 0\n",
    "\n",
    "        if loc.defect & seq.Location.Defect.MISS_RIGHT:\n",
    "            # If the feature extends into the previous or next line\n",
    "            # do not draw an arrow head\n",
    "            draw_head = False\n",
    "        else:\n",
    "            draw_head = True\n",
    "\n",
    "        axes.add_patch(\n",
    "            biotite.AdaptiveFancyArrow(\n",
    "                x,\n",
    "                y,\n",
    "                dx,\n",
    "                dy,\n",
    "                self._tail_width * bbox.height,\n",
    "                self._head_width * bbox.height,\n",
    "                # Create head with 90 degrees tip\n",
    "                # -> head width/length ratio = 1/2\n",
    "                head_ratio=0.5,\n",
    "                draw_head=draw_head,\n",
    "                color=biotite.colors[\"orange\"],\n",
    "                linewidth=0,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "# Converter for the DSSP secondary structure elements\n",
    "# to the classical ones\n",
    "dssp_to_abc = {\n",
    "    \"I\": \"c\",\n",
    "    \"S\": \"c\",\n",
    "    \"H\": \"a\",\n",
    "    \"E\": \"b\",\n",
    "    \"G\": \"c\",\n",
    "    \"B\": \"b\",\n",
    "    \"T\": \"c\",\n",
    "    \"C\": \"c\",\n",
    "}\n",
    "\n",
    "\n",
    "def visualize_secondary_structure(sse, first_id):\n",
    "    \"\"\"\n",
    "    Helper function to convert secondary structure array to annotation\n",
    "    and visualize it.\n",
    "    \"\"\"\n",
    "\n",
    "    def _add_sec_str(annotation, first, last, str_type):\n",
    "        if str_type == \"a\":\n",
    "            str_type = \"helix\"\n",
    "        elif str_type == \"b\":\n",
    "            str_type = \"sheet\"\n",
    "        else:\n",
    "            # coil\n",
    "            return\n",
    "        feature = seq.Feature(\n",
    "            \"SecStr\", [seq.Location(first, last)], {\"sec_str_type\": str_type}\n",
    "        )\n",
    "        annotation.add_feature(feature)\n",
    "\n",
    "    # Find the intervals for each secondary structure element\n",
    "    # and add to annotation\n",
    "    annotation = seq.Annotation()\n",
    "    curr_sse = None\n",
    "    curr_start = None\n",
    "    for i in range(len(sse)):\n",
    "        if curr_start is None:\n",
    "            curr_start = i\n",
    "            curr_sse = sse[i]\n",
    "        else:\n",
    "            if sse[i] != sse[i - 1]:\n",
    "                _add_sec_str(\n",
    "                    annotation, curr_start + first_id, i - 1 + first_id, curr_sse\n",
    "                )\n",
    "                curr_start = i\n",
    "                curr_sse = sse[i]\n",
    "    # Add last secondary structure element to annotation\n",
    "    _add_sec_str(annotation, curr_start + first_id, i + first_id, curr_sse)\n",
    "\n",
    "    fig = plt.figure(figsize=(30.0, 3.0))\n",
    "    ax = fig.add_subplot(111)\n",
    "    graphics.plot_feature_map(\n",
    "        ax,\n",
    "        annotation,\n",
    "        symbols_per_line=150,\n",
    "        loc_range=(first_id, first_id + len(sse)),\n",
    "        feature_plotters=[HelixPlotter(), SheetPlotter()],\n",
    "    )\n",
    "    fig.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_ss8(ss8_string):\n",
    "    ss3 = np.array([dssp_to_abc[e] for e in ss8_string], dtype=\"U1\")\n",
    "    _, ax = visualize_secondary_structure(ss3, 1)\n",
    "    ax.set_xticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5117db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function annotation track\n",
    "# Functions for visualizing InterPro function annotations\n",
    "\n",
    "from dna_features_viewer import GraphicFeature, GraphicRecord\n",
    "from matplotlib import colormaps\n",
    "\n",
    "from esm.utils.function.interpro import InterPro, InterProEntryType\n",
    "from esm.sdk.api import FunctionAnnotation\n",
    "\n",
    "def visualize_function_annotations(\n",
    "    annotations: list[FunctionAnnotation],\n",
    "    sequence_length: int,\n",
    "    ax: plt.Axes,\n",
    "    interpro_=InterPro(),\n",
    "):\n",
    "    cmap = colormaps[\"tab10\"]\n",
    "    colors = [cmap(i) for i in range(len(InterProEntryType))]\n",
    "    type_colors = dict(zip(InterProEntryType, colors))\n",
    "\n",
    "    features = []\n",
    "    for annotation in annotations:\n",
    "        if annotation.label in interpro_.entries:\n",
    "            entry = interpro_.entries[annotation.label]\n",
    "            label = entry.name\n",
    "            entry_type = entry.type\n",
    "        else:\n",
    "            label = annotation.label\n",
    "            entry_type = InterProEntryType.UNKNOWN\n",
    "\n",
    "        feature = GraphicFeature(\n",
    "            start=annotation.start - 1,  # one index -> zero index\n",
    "            end=annotation.end,\n",
    "            label=label,\n",
    "            color=type_colors[entry_type],\n",
    "            strand=None,\n",
    "        )\n",
    "        features.append(feature)\n",
    "\n",
    "    record = GraphicRecord(\n",
    "        sequence=None, sequence_length=sequence_length, features=features\n",
    "    )\n",
    "\n",
    "    record.plot(figure_width=12, plot_sequence=False, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c60b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D STRUCTURE TRACK \n",
    "from biotite.database import rcsb\n",
    "from esm.sdk.api import ESMProtein\n",
    "from esm.utils.structure.protein_chain import ProteinChain\n",
    "from esm.utils.types import FunctionAnnotation\n",
    "pdb_id = \"1cm4\"\n",
    "chain_id = \"A\"\n",
    "\n",
    "# Create a protein using a pdb format file from RCSB\n",
    "# Note: instead of the next two lines, we could use\n",
    "# protein_chain = ProteinChain.from_rcsb(pdb_id, chain_id)\n",
    "# but in future implementations, this function may use the mmcif file\n",
    "# which would throw off some indices later on in this notebook\n",
    "str_io = rcsb.fetch(pdb_id, \"pdb\")\n",
    "protein_chain = ProteinChain.from_pdb(str_io, chain_id=chain_id, id=pdb_id)\n",
    "protein = ESMProtein.from_protein_chain(protein_chain)\n",
    "\n",
    "## We can also load from a local pdb file by passing its path\n",
    "# protein_chain = ProteinChain.from_pdb('xxxx.pdb', chain_id=chain_id, id=pdb_id)\n",
    "# The chain_id and id arguments are optional and will be inferred if None\n",
    "print(protein.sequence)\n",
    "print(protein.coordinates.shape)\n",
    "visualize_3D_coordinates(protein.coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa72840",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SECONDARY STRUCTURE TRACK \n",
    "from biotite.structure import annotate_sse\n",
    "def get_approximate_ss(protein_chain: ProteinChain):\n",
    "    # get biotite's ss3 representation\n",
    "    ss3_arr = annotate_sse(protein_chain.atom_array)\n",
    "    biotite_ss3_str = \"\".join(ss3_arr)\n",
    "\n",
    "    # translate into ESM3's representation\n",
    "    translation_table = str.maketrans(\n",
    "        {\n",
    "            \"a\": \"H\",  # alpha helix\n",
    "            \"b\": \"E\",  # beta sheet\n",
    "            \"c\": \"C\",  # coil\n",
    "        }\n",
    "    )\n",
    "    esm_ss3 = biotite_ss3_str.translate(translation_table)\n",
    "    return esm_ss3\n",
    "\n",
    "protein.secondary_structure = get_approximate_ss(protein_chain)\n",
    "print(protein.secondary_structure)\n",
    "plot_ss8(protein.secondary_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43585a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION TRACK \n",
    "interpro_function_annotations = [\n",
    "    FunctionAnnotation(label=\"IPR050145\", start=1, end=142),  # 1 indexed, inclusive;\n",
    "    FunctionAnnotation(label=\"IPR002048\", start=4, end=75),\n",
    "    FunctionAnnotation(label=\"IPR002048\", start=77, end=144),\n",
    "    FunctionAnnotation(label=\"IPR011992\", start=1, end=143),\n",
    "    FunctionAnnotation(label=\"IPR018247\", start=17, end=29),\n",
    "    FunctionAnnotation(label=\"IPR018247\", start=53, end=65),\n",
    "    FunctionAnnotation(label=\"IPR018247\", start=90, end=102),\n",
    "    FunctionAnnotation(label=\"IPR018247\", start=126, end=138),\n",
    "]\n",
    "fig, ax = plt.subplots(figsize=(20.0, 4.0))\n",
    "visualize_function_annotations(interpro_function_annotations, len(protein), ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca08c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function keywords\n",
    "from esm.tokenization import InterProQuantizedTokenizer\n",
    "\n",
    "\n",
    "def get_keywords_from_interpro(\n",
    "    interpro_annotations,\n",
    "    interpro2keywords=InterProQuantizedTokenizer().interpro2keywords,\n",
    "):\n",
    "    keyword_annotations_list = []\n",
    "    for interpro_annotation in interpro_annotations:\n",
    "        keywords = interpro2keywords.get(interpro_annotation.label, [])\n",
    "        keyword_annotations_list.extend(\n",
    "            [\n",
    "                FunctionAnnotation(\n",
    "                    label=keyword,\n",
    "                    start=interpro_annotation.start,\n",
    "                    end=interpro_annotation.end,\n",
    "                )\n",
    "                for keyword in keywords\n",
    "            ]\n",
    "        )\n",
    "    return keyword_annotations_list\n",
    "\n",
    "protein.function_annotations = get_keywords_from_interpro(interpro_function_annotations)\n",
    "protein.function_annotations\n",
    "fig, ax = plt.subplots(figsize=(20.0, 8.0))\n",
    "visualize_function_annotations(protein.function_annotations, len(protein), ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2347e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SASA PLOT\n",
    "cmap = colormaps[\"cividis\"]\n",
    "clip_sasa_lower = 10\n",
    "clip_sasa_upper = 90\n",
    "\n",
    "\n",
    "def plot_heatmap_legend(cmap, clip_sasa_lower, clip_sasa_upper):\n",
    "    gradient = np.linspace(0, 1, 256)\n",
    "    gradient = np.vstack((gradient, gradient))\n",
    "    _, ax = plt.subplots(figsize=(5, 0.3), dpi=350)\n",
    "    ax.imshow(gradient, aspect=\"auto\", cmap=cmap)\n",
    "    ax.text(\n",
    "        0.1,\n",
    "        -0.3,\n",
    "        f\"{clip_sasa_lower} or lower\",\n",
    "        va=\"center\",\n",
    "        ha=\"right\",\n",
    "        fontsize=7,\n",
    "        transform=ax.transAxes,\n",
    "    )\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        -0.3,\n",
    "        f\"{(clip_sasa_lower + clip_sasa_upper) // 2}\",\n",
    "        va=\"center\",\n",
    "        ha=\"right\",\n",
    "        fontsize=7,\n",
    "        transform=ax.transAxes,\n",
    "    )\n",
    "    ax.text(\n",
    "        0.9,\n",
    "        -0.3,\n",
    "        f\"{clip_sasa_upper} or higher\",\n",
    "        va=\"center\",\n",
    "        ha=\"left\",\n",
    "        fontsize=7,\n",
    "        transform=ax.transAxes,\n",
    "    )\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_heatmap_legend(cmap, clip_sasa_lower, clip_sasa_upper)\n",
    "\n",
    "# Functions for visualizing SASA as colors on the 3D structure\n",
    "\n",
    "\n",
    "def get_color_strings(sasa, clip_sasa_lower, clip_sasa_upper, cmap):\n",
    "    transformed_sasa = np.clip(sasa, clip_sasa_lower, clip_sasa_upper)\n",
    "    transformed_sasa = (transformed_sasa - clip_sasa_lower) / (\n",
    "        clip_sasa_upper - clip_sasa_lower\n",
    "    )\n",
    "    rgbas = (cmap(transformed_sasa) * 255).astype(int)\n",
    "\n",
    "    return [f\"rgb({rgba[0]},{rgba[1]},{rgba[2]})\" for rgba in rgbas]\n",
    "\n",
    "\n",
    "def visualize_sasa_3D_protein(\n",
    "    protein, clip_sasa_lower=clip_sasa_lower, clip_sasa_upper=clip_sasa_upper, cmap=cmap\n",
    "):\n",
    "    pdb_string = protein.to_pdb_string()\n",
    "    plot_heatmap_legend(cmap, clip_sasa_lower, clip_sasa_upper)\n",
    "    view = py3Dmol.view(width=400, height=400)\n",
    "    view.addModel(pdb_string, \"pdb\")\n",
    "\n",
    "    for res_pos, res_color in enumerate(\n",
    "        get_color_strings(protein.sasa, clip_sasa_lower, clip_sasa_upper, cmap)\n",
    "    ):\n",
    "        view.setStyle(\n",
    "            {\"chain\": \"A\", \"resi\": res_pos + 1}, {\"cartoon\": {\"color\": res_color}}\n",
    "        )\n",
    "    view.zoomTo()\n",
    "    view.render()\n",
    "    view.center()\n",
    "    return view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77fa932",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SASA Track \n",
    "protein.sasa = protein_chain.sasa()\n",
    "plt.plot(protein.sasa)\n",
    "visualize_sasa_3D_protein(protein)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8913ff",
   "metadata": {},
   "source": [
    "Tut2: ESMC Embedding + threaded async forge\n",
    "\n",
    "Since we're embedding more than a few sequences, we're going to use a threaded async call to Forge and let Forge take care of batching and parallelization on the backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ecb9543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MEYLAPTSFTTLLSFTASLLVLAIILFYFIQSHKNVKKHPLPPGPKRWPIVGCLPTMLRNKPVYRWIHNLMKEMNTEIACIRLGNVHVIPVICPDIACEFLKAQDNTFASRPHTMTTDLISRGYLTTALSPSGDQWNKMKKVLMTHVLSPKKHQWLYSKRVEEADHLVHYVYNQCKKSVHQGGIVNLRTAAQHYCANLTRKMLFNKRFFGEGMKDGGPGFEEEEYVDALFSCLNHIYAFCISDFLPSLIGLDLDGHEKVVMENHRIINKYHDPIIHERVQQWKDGAKKDTEDLLDILITLKDPDGNPLLSKDEIKAQITEIMVAAVDNPSNACEWAFAEMLNQPEILEKATEELDRVVGKERLVQESDFAHLNYVKACAREAFRLHPVAPFNVPHVSAADTTVANYFIPKGSYVLLSRLGLGRNPKVWDEPLKFKPERHLNEMEKVVLTENNLRFISFSTGKRGCIGVTLGTSMTTMLFARLLQAFTWSLPPSQSSIDLTIAEDSMALAKPLSALAKPRLPPQVYPGY', 'MSSSSSSSTSMIDLMAAIIKGEPVIVSDPANASAYESVAAELSSMLIENRQFAMIVTTSIAVLIGCIVMLVWRRSGSGNSKRVEPLKPLVIKPREEEIDDGRKKVTIFFGTQTGTAEGFAKALGEEAKARYEKTRFKIVDLDDYAADDDEYEEKLKKEDVAFFFLATYGDGEPTDNAARFYKWFTEGNDRGEWLKNLKYGVFGLGNRQYEHFNKVAKVVDDILVEQGAQRLVQVGLGDDDQCIEDDFTAWREALWPELDTILREEGDTAVATPYTAAVLEYRVSIHDSEDAKFNDINMANGNGYTVFDAQHPYKANVAVKRELHTPESDRSCIHLEFDIAGSGLTYETGDHVGVLCDNLSETVDEALRLLDMSPDTYFSLHAEKEDGTPISSSLPPPFPPCNLRTALTRYACLLSSPKKSALVALAAHASDPTEAERLKHLASPAGKDEYSKWVVESQRSLLEVMAEFPSAKPPLGVFFAGVAPRLQPRFYSISSSPKIAETRIHVTCALVYEKMPTGRIHKGVCSTWMKNAVPYEKSENCSSAPIFVRQSNFKLPSDSKVPIIMIGPGTGLAPFRGFLQERLALVESGVELGPSVLFFGCRNRRMDFIYEEELQRFVESGALAELSVAFSREGPTKEYVQHKMMDKASDIWNMISQGAYLYVCGDAKGMARDVHRSLHTIAQEQGSMDSTKAEGFVKNLQTSGRYLRDVW', 'MWKLKIAEGGSPWLRTTNNHVGRQFWEFDPNLGTPEDLAAVEEARKSFSDNRFVQKHSADLLMRLQFSRENLISPVLPQVKIEDTDDVTEEMVETTLKRGLDFYSTIQAHDGHWPGDYGGPMFLLPGLIITLSITGALNTVLSEQHKQEMRRYLYNHQNEDGGWGLHIEGPSTMFGSVLNYVTLRLLGEGPNDGDGDMEKGRDWILNHGGATNITSWGKMWLSVLGAFEWSGNNPLPPEIWLLPYFLPIHPGRMWCHCRMVYLPMSYLYGKRFVGPITSTVLSLRKELFTVPYHEVNWNEARNLCAKEDLYYPHPLVQDILWASLHKIVEPVLMRWPGANLREKAIRTAIEHIHYEDENTRYICIGPVNKVLNMLCCWVEDPNSEAFKLHLPRIHDFLWLAEDGMKMQGYNGSQLWDTGFAIQAILATNLVEEYGPVLEKAHSFVKNSQVLEDCPGDLNYWYRHISKGAWPFSTADHGWPISDCTAEGLKAALLLSKVPKAIVGEPIDAKRLYEAVNVIISLQNADGGLATYELTRSYPWLELINPAETFGDIVIDYPYVECTSAAIQALISFRKLYPGHRKKEVDECIEKAVKFIESIQAADGSWYGSWAVCFTYGTWFGVKGLVAVGKTLKNSPHVAKACEFLLSKQQPSGGWGESYLSCQDKVYSNLDGNRSHVVNTAWAMLALIGAGQAEVDRKPLHRAARYLINAQMENGDFPQQEIMGVFNRNCMITYAAYRNIFPIWALGEYRCQVLLQQGE', 'MSGSSSPSLWLAPNPSKRWGELFFLFYTPFWLTLCLGIVVPYKLYETFTELEYLLLALVSAVPAFVIPMLLVGKADRSLCWKDRYWVKANLWIIVFSYVGNYFWTHYFFKVLGASYTFPSWKMNNVPHTTFFLTHVCFLFYHVASNITLRRLRHSTADLPDSLKWCFEAAWILALSYFIAYLETIAIANFPYYEFVDRSAMYRVGCLFYAIYFIVSFPMFFRMDEKSTDEWDLSRVAVDALGAAMLVTIILDLWRLFLGPIVPLPEGQNCLQSGLPWFSN', 'MEEIADKTFFRYCLLTLIFAGPPTAVLLKFLQAPYGKHNRTGWGPTVSPPIAWFVMESPTLWLTLLLFPFGRHALNPKSLLLFSPYLIHYFHRTIIYPLRLFRSSFPAGKNGFPITIAALAFTFNLLNGYIQARWVSHYKDDYEDGNWFWWRFVIGMVVFITGMYINITSDRTLVRLKKENRGGYVIPRGGWFELVSCPNYFGEAIEWLGWAVMTWSWAGIGFFLYTCSNLFPRARASHKWYIAKFKEEYPKTRKAVIPFVY', 'MSDLQTPLVRPKRKKTWVDYFVKFRWIIVIFIVLPFSATFYFLIYLGDMWSESKSFEKRQKEHDENVKKVIKRLKGRDASKDGLVCTARKPWIAVGMRNVDYKRARHFEVDLGEFRNILEINKEKMTARVEPLVNMGQISRATVPMNLSLAVVAELDDLTVGGLINGYGIEGSSHIYGLFADTVEAYEIVLAGGELVRATRDNEYSDLYYAIPWSQGTLGLLVAAEIRLIKVKEYMRLTYIPVKGDLQALAQGYIDSFAPKDGDKSKIPDFVEGMVYNPTEGVMMVGTYASKEEAKKKGNKINNVGWWFKPWFYQHAQTALKKGQFVEYIPTREYYHRHTRCLYWEGKLILPFGDQFWFRYLLGWLMPPKVSLLKATQGEAIRNYYHDMHVIQDMLVPLYKVGDALEWVHREMEVYPIWLCPHKLFKQPIKGQIYPEPGFEYENRQGDTEDAQMYTDVGVYYAPGCVLRGEEFDGSEAVRRMEKWLIENHGFQPQYAVSELDEKSFWRMFNGELYEECRKKYRAIGTFMSVYYKSKKGRKTEKEVREAEQAHLETAYAEAD', 'MFETEHHTLLPLLLLPSLLSLLLFLILLKRRNRKTRFNLPPGKSGWPFLGETIGYLKPYTATTLGDFMQQHVSKYGKIYRSNLFGEPTIVSADAGLNRFILQNEGRLFECSYPRSIGGILGKWSMLVLVGDMHRDMRSISLNFLSHARLRTILLKDVERHTLFVLDSWQQNSIFSAQDEAKKFTFNLMAKHIMSMDPGEEETEQLKKEYVTFMKGVVSAPLNLPGTAYHKALQSRATILKFIERKMEERKLDIKEEDQEEEEVKTEDEAEMSKSDHVRKQRTDDDLLGWVLKHSNLSTEQILDLILSLLFAGHETSSVAIALAIFFLQACPKAVEELREEHLEIARAKKELGESELNWDDYKKMDFTQCVINETLRLGNVVRFLHRKALKDVRYKGYDIPSGWKVLPVISAVHLDNSRYDQPNLFNPWRWQQQNNGASSSGSGSFSTWGNNYMPFGGGPRLCAGSELAKLEMAVFIHHLVLKFNWELAEDDKPFAFPFVDFPNGLPIRVSRIL', 'MIPYATVEEASIALGRNLTRLETLWFDYSATKSDYYLYCHNILFLFLVFSLVPLPLVFVELARSASGLFNRYKIQPKVNYSLSDMFKCYKDVMTMFILVVGPLQLVSYPSIQMIEIRSGLPLPTITEMLSQLVVYFLIEDYTNYWVHRFFHSKWGYDKIHRVHHEYTAPIGYAAPYAHWAEVLLLGIPTFMGPAIAPGHMITFWLWIALRQMEAIETHSGYDFPWSPTKYIPFYGGAEYHDYHHYVGGQSQSNFASVFTYCDYIYGTDKGYRFQKKLLEQIKESSKKSNKHNGGIKSD']\n",
      "embedding shape [num_layers, hidden_size]: torch.Size([530, 960])\n"
     ]
    }
   ],
   "source": [
    "#setup\n",
    "from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
    "import pandas as pd \n",
    "from esm.sdk import client\n",
    "import os\n",
    "#token = os.getenv(\"ESM_API_KEY\")\n",
    "model = client(\n",
    "    model=\"esmc-300m-2024-12\", url=\"https://forge.evolutionaryscale.ai\", token=\"1hM3JVqy7zILco2fwPgF9v\"\n",
    ")\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Sequence\n",
    "\n",
    "from esm.sdk.api import (\n",
    "    ESM3InferenceClient,\n",
    "    ESMProtein,\n",
    "    ESMProteinError,\n",
    "    LogitsConfig,\n",
    "    LogitsOutput,\n",
    "    ProteinType,\n",
    ")\n",
    "\n",
    "EMBEDDING_CONFIG = LogitsConfig(\n",
    "    sequence=True, return_embeddings=True, return_hidden_states=True, ith_hidden_layer=12\n",
    ")\n",
    "\n",
    "\n",
    "def embed_sequence(model: ESM3InferenceClient, sequence: str) -> LogitsOutput:\n",
    "    protein = ESMProtein(sequence=sequence)\n",
    "    protein_tensor = model.encode(protein)\n",
    "    output = model.logits(protein_tensor, EMBEDDING_CONFIG)\n",
    "    return output\n",
    "\n",
    "\n",
    "def batch_embed(\n",
    "    model: ESM3InferenceClient, inputs: Sequence[ProteinType]\n",
    ") -> Sequence[LogitsOutput]:\n",
    "    \"\"\"Forge supports auto-batching. So batch_embed() is as simple as running a collection\n",
    "    of embed calls in parallel using asyncio.\n",
    "    \"\"\"\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [\n",
    "            executor.submit(embed_sequence, model, protein) for protein in inputs\n",
    "        ]\n",
    "        results = []\n",
    "        for future in futures:\n",
    "            try:\n",
    "                results.append(future.result())\n",
    "            except Exception as e:\n",
    "                results.append(ESMProteinError(500, str(e)))\n",
    "    return results\n",
    "\n",
    "\n",
    "#Request specific hidden layer\n",
    "#ESM C 6B's hidden states are really large, so we only allow one specific layer to be requested per API call. This also works for other ESM C models, but it is required for ESM C 6B. Refer to https://forge.evolutionaryscale.ai/console to find the number of hidden layers for each model.\n",
    "# ESMC_6B_EMBEDDING_CONFIG = LogitsConfig(return_hidden_states=True, ith_hidden_layer=55)\n",
    "\n",
    "#load example sequence dataset\n",
    "def read_fasta_fast(fasta_file):\n",
    "    records = []\n",
    "    with open(fasta_file) as in_handle:\n",
    "        for title, seq in SimpleFastaParser(in_handle):\n",
    "            gene_id = title.split()[0]\n",
    "            records.append((gene_id, seq, len(seq)))\n",
    "    return pd.DataFrame(records, columns=[\"Gene\", \"Sequence\", \"Length\"])\n",
    "x = read_fasta_fast(\"esmtest.fasta\")\n",
    "seq = x[\"Sequence\"].to_list()\n",
    "print(seq)\n",
    "#output = embed_sequence(model, seq))\n",
    "output = batch_embed(model, seq)\n",
    "\n",
    "\n",
    "import torch\n",
    "# we'll summarize the embeddings using their mean across the sequence dimension\n",
    "# which allows us to compare embeddings for sequences of different lengths\n",
    "all_mean_embeddings = [\n",
    "    torch.mean(o.hidden_states, dim=0).squeeze().cpu()\n",
    "    for o in output\n",
    "]\n",
    "# now we have a list of tensors of [num_layers, hidden_size]\n",
    "print(\"embedding shape [num_layers, hidden_size]:\", all_mean_embeddings[0].shape)\n",
    "\n",
    "#note: TAKES TOO MANY CREDITS AND TOO LONG? \n",
    "# ok now it worked because i properly used the input sequence (not a df object)\n",
    "# and then specified in the embedding config to return a specific hidden layer\n",
    "# works with both esm3 and esmc model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a3edaa",
   "metadata": {},
   "source": [
    "Examine the performance of different layer embeddings: \n",
    "\n",
    "For this example, we're going to use PCA to visualize whether the embeddings separate our proteins by their structural class. To assess the quality of our PCA, we fit a K means classifier with three clusters, corresponding to the three structural classes of our enzyme, and compute the rand index, a measure of the quality of the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f44192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np  # ✅ add these imports for labels\n",
    "\n",
    "# --- Step 1: Create dummy labels for testing ---\n",
    "n_sequences = len(all_mean_embeddings)\n",
    "labels = [\n",
    "    \"CYP79D6v3\",          \n",
    "    \"CPR\",               \n",
    "    \"CAS1\", \n",
    "    \"CPI1\",\n",
    "    \"DET2\",\n",
    "    \"DIM\",\n",
    "    \"CYP90B1\",\n",
    "    \"SMO1-1\"\n",
    "]\n",
    "df = pd.DataFrame({\"lid_type\": labels})\n",
    "# ✅ now df[\"lid_type\"] exists globally and your function can use it\n",
    "\n",
    "N_KMEANS_CLUSTERS = 3\n",
    "\n",
    "# --- Step 2: Your plotting function ---\n",
    "def plot_embeddings_at_layer(all_mean_embeddings: list, layer_idx: int):\n",
    "    # Stack embeddings across sequences\n",
    "    stacked_mean_embeddings = torch.stack(\n",
    "        [embedding[layer_idx, :] for embedding in all_mean_embeddings]\n",
    "    )\n",
    "    \n",
    "    # ✅ Cast to float32 before converting to numpy\n",
    "    stacked_mean_embeddings = stacked_mean_embeddings.float().cpu().numpy()\n",
    "\n",
    "    # project all the embeddings to 2D using PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(stacked_mean_embeddings)\n",
    "    projected_mean_embeddings = pca.transform(stacked_mean_embeddings)\n",
    "\n",
    "    # compute kmeans purity as a measure of how good the clustering is\n",
    "    kmeans = KMeans(n_clusters=N_KMEANS_CLUSTERS, random_state=0).fit(\n",
    "        projected_mean_embeddings\n",
    "    )\n",
    "    rand_index = adjusted_rand_score(df[\"lid_type\"], kmeans.labels_)\n",
    "\n",
    "    # plot the clusters\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    sns.scatterplot(\n",
    "        x=projected_mean_embeddings[:, 0],\n",
    "        y=projected_mean_embeddings[:, 1],\n",
    "        hue=df[\"lid_type\"],\n",
    "    )\n",
    "    plt.title(\n",
    "        f\"PCA of mean embeddings at layer {layer_idx}.\\nRand index: {rand_index:.2f}\"\n",
    "    )\n",
    "\n",
    "    for i, lab in enumerate(labels):\n",
    "        plt.text(projected_mean_embeddings[i,0] + 1,\n",
    "             projected_mean_embeddings[i,1] + 1,\n",
    "             f\"{i+1}:{lab}\", fontsize=8)\n",
    "    \n",
    "    plt.xlabel(\"PC 1\")\n",
    "    plt.ylabel(\"PC 2\")\n",
    "    plt.show()\n",
    "\n",
    "# --- Step 3: Call your function ---\n",
    "plot_embeddings_at_layer(all_mean_embeddings, layer_idx=30)\n",
    "plot_embeddings_at_layer(all_mean_embeddings, layer_idx=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3ed82c",
   "metadata": {},
   "source": [
    "Small exercise - Test each layer in a classifier of enzyme class to see which gives best performance. Not fine-tuned, just a classifier layer on top means fast, less compute, no risk of catastrophic forgetting, can use very large ESM-3 models on modest GPUs. But might underperform vs fine-tuning. If compute allows → try fine-tuning last 2–4 layers of ESM-3 with small LR (not whole model). This is often called “partial fine-tuning” or “adapter tuning.”  Gives you most of the gain without exploding GPU needs.\n",
    "\n",
    "Example NetsolP:\n",
    "\n",
    "The output representations of each amino acid in the sequence are averaged to represent the protein and a linear classification layer is used to predict binary solubility.\n",
    "\n",
    "The trained models have a suffix ‘-F’ and ‘-P’ to indicate whether they are trained end-to-end (fine-tuning) or only the classification layer (pretrained embedding)\n",
    "\n",
    "The maximum sequence length used for training is 510, by removing around 3.4% of the training sequences that exceed this length, to speed up the training process.For prediction, amino acids after position 1022 are removed due to the maximum length constraints of the transformer models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a655dbe",
   "metadata": {},
   "source": [
    "TUT3 - GFP DESIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a8437d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "import biotite.sequence as seq\n",
    "import biotite.sequence.align as align\n",
    "import biotite.sequence.graphics as graphics\n",
    "import matplotlib.pyplot as pl\n",
    "import py3Dmol\n",
    "import torch\n",
    "\n",
    "from esm.sdk import client\n",
    "from esm.sdk.api import ESMProtein, GenerationConfig\n",
    "from esm.utils.structure.protein_chain import ProteinChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d86699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
